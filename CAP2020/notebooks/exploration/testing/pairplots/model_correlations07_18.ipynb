{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECK CORRELATIONS AND DISTRIBUTIONS BETWEEN DAILY RETURNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import os\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import matplotlib.dates as dates\n",
    "from datetime import datetime\n",
    "import matplotlib.ticker as ticker\n",
    "from random import shuffle\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\A Sua\\\\Documents\\\\AWS\\\\credentials')\n",
    "list_of_lines = []\n",
    "with open('capstoner_key.txt') as fp:\n",
    "    lines = fp.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        list_of_lines.append(line)\n",
    "    fp.close()\n",
    "\n",
    "\n",
    "location = list_of_lines[0].split('=')[1]\n",
    "AWS_ACCESS_KEY_ID = list_of_lines[1].split('=')[1]\n",
    "AWS_SECRET_ACCESS_KEY = list_of_lines[2].split('=')[1]\n",
    "os.chdir('C:\\\\Users\\\\A Sua\\\\Documents\\\\FIU\\\\CAP2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df_name_list = ['df_bidlo', 'df_askhi', 'df_price', 'df_volume', 'df_returns', 'df_shares_out', 'df_spread', 'df_market_cap']\n",
    "\n",
    "## lets keep: 'df_price', 'df_volume', 'df_returns', 'df_shares_out', 'df_spread', 'df_market_cap'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### marketcap and shares out will have repeating values for most days, this could weigh on the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### should we build sliding window classifiers? like 1 classifier per company to denote a customized classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BUCKETS & KEYS\n",
    "BUCKET_NAME = 'capstonefiu2020-data'\n",
    "key_location = 'data/processed'\n",
    "file_name = 'df_price07_18.csv'\n",
    "key_path = key_location + '/' + file_name\n",
    "\n",
    "### ACTIVATE CLIENT\n",
    "client = boto3.client('s3', \n",
    "              aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
    "               aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "\n",
    "### INSTANTIATE DATAFRAME\n",
    "csv_obj = client.get_object(Bucket = BUCKET_NAME, Key=key_path)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')\n",
    "\n",
    "df_price = pd.read_csv(StringIO(csv_string))\n",
    "df_price.index = df_price['Unnamed: 0']\n",
    "df_price = df_price.drop('Unnamed: 0', axis=1)\n",
    "df_price.index.names = ['date']\n",
    "\n",
    "\n",
    "#df_bidlo.head()\n",
    "#### TRANSPOSE DATA FOR EASE OF USE\n",
    "#df_returns = df_bidlo.T\n",
    "#df_returns.index = pd.to_datetime(df_returns.index)\n",
    "#print(df_price.shape)\n",
    "df_price.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import volume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BUCKETS & KEYS\n",
    "BUCKET_NAME = 'capstonefiu2020-data'\n",
    "key_location = 'data/processed'\n",
    "file_name = 'df_volume07_18.csv'\n",
    "key_path = key_location + '/' + file_name\n",
    "\n",
    "### ACTIVATE CLIENT\n",
    "client = boto3.client('s3', \n",
    "              aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
    "               aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "\n",
    "### INSTANTIATE DATAFRAME\n",
    "csv_obj = client.get_object(Bucket = BUCKET_NAME, Key=key_path)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')\n",
    "\n",
    "df_volume = pd.read_csv(StringIO(csv_string))\n",
    "df_volume.index = df_volume['Unnamed: 0']\n",
    "df_volume = df_volume.drop('Unnamed: 0', axis=1)\n",
    "df_volume.index.names = ['date']\n",
    "\n",
    "\n",
    "#df_bidlo.head()\n",
    "#### TRANSPOSE DATA FOR EASE OF USE\n",
    "#df_returns = df_bidlo.T\n",
    "#df_returns.index = pd.to_datetime(df_returns.index)\n",
    "print(df_volume.shape)\n",
    "df_volume.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BUCKETS & KEYS\n",
    "BUCKET_NAME = 'capstonefiu2020-data'\n",
    "key_location = 'data/processed'\n",
    "file_name = 'df_returns07_18.csv'\n",
    "key_path = key_location + '/' + file_name\n",
    "\n",
    "### ACTIVATE CLIENT\n",
    "client = boto3.client('s3', \n",
    "              aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
    "               aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "\n",
    "### INSTANTIATE DATAFRAME\n",
    "csv_obj = client.get_object(Bucket = BUCKET_NAME, Key=key_path)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')\n",
    "\n",
    "df_returns = pd.read_csv(StringIO(csv_string))\n",
    "df_returns.index = df_returns['Unnamed: 0']\n",
    "df_returns = df_returns.drop('Unnamed: 0', axis=1)\n",
    "df_returns.index.names = ['date']\n",
    "\n",
    "\n",
    "#df_bidlo.head()\n",
    "#### TRANSPOSE DATA FOR EASE OF USE\n",
    "#df_returns = df_bidlo.T\n",
    "#df_returns.index = pd.to_datetime(df_returns.index)\n",
    "df_returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import shares out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BUCKETS & KEYS\n",
    "BUCKET_NAME = 'capstonefiu2020-data'\n",
    "key_location = 'data/processed'\n",
    "file_name = 'df_shares_out07_18.csv'\n",
    "key_path = key_location + '/' + file_name\n",
    "\n",
    "### ACTIVATE CLIENT\n",
    "client = boto3.client('s3', \n",
    "              aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
    "               aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "\n",
    "### INSTANTIATE DATAFRAME\n",
    "csv_obj = client.get_object(Bucket = BUCKET_NAME, Key=key_path)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')\n",
    "\n",
    "df_sharesout = pd.read_csv(StringIO(csv_string))\n",
    "df_sharesout.index = df_sharesout['Unnamed: 0']\n",
    "df_sharesout = df_sharesout.drop('Unnamed: 0', axis=1)\n",
    "df_sharesout.index.names = ['date']\n",
    "\n",
    "\n",
    "#df_bidlo.head()\n",
    "#### TRANSPOSE DATA FOR EASE OF USE\n",
    "#df_returns = df_bidlo.T\n",
    "#df_returns.index = pd.to_datetime(df_returns.index)\n",
    "df_sharesout.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BUCKETS & KEYS\n",
    "BUCKET_NAME = 'capstonefiu2020-data'\n",
    "key_location = 'data/processed'\n",
    "file_name = 'df_spread07_18.csv'\n",
    "key_path = key_location + '/' + file_name\n",
    "\n",
    "### ACTIVATE CLIENT\n",
    "client = boto3.client('s3', \n",
    "              aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
    "               aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "\n",
    "### INSTANTIATE DATAFRAME\n",
    "csv_obj = client.get_object(Bucket = BUCKET_NAME, Key=key_path)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')\n",
    "\n",
    "df_spread = pd.read_csv(StringIO(csv_string))\n",
    "df_spread.index = df_spread['Unnamed: 0']\n",
    "df_spread = df_spread.drop('Unnamed: 0', axis=1)\n",
    "df_spread.index.names = ['date']\n",
    "\n",
    "\n",
    "#df_bidlo.head()\n",
    "#### TRANSPOSE DATA FOR EASE OF USE\n",
    "#df_market_cap = df_market_cap.T\n",
    "# df_market_cap.index = pd.to_datetime(df_bidlo.index)\n",
    "df_spread.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import marketcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BUCKETS & KEYS\n",
    "BUCKET_NAME = 'capstonefiu2020-data'\n",
    "key_location = 'data/processed'\n",
    "file_name = 'df_market_cap07_18.csv'\n",
    "key_path = key_location + '/' + file_name\n",
    "\n",
    "### ACTIVATE CLIENT\n",
    "client = boto3.client('s3', \n",
    "              aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
    "               aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "\n",
    "### INSTANTIATE DATAFRAME\n",
    "csv_obj = client.get_object(Bucket = BUCKET_NAME, Key=key_path)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')\n",
    "\n",
    "df_marketcap = pd.read_csv(StringIO(csv_string))\n",
    "df_marketcap.index = df_marketcap['Unnamed: 0']\n",
    "df_marketcap = df_marketcap.drop('Unnamed: 0', axis=1)\n",
    "df_marketcap.index.names = ['date']\n",
    "\n",
    "\n",
    "#df_bidlo.head()\n",
    "#### TRANSPOSE DATA FOR EASE OF USE\n",
    "#df_market_cap = df_market_cap.T\n",
    "# df_market_cap.index = pd.to_datetime(df_bidlo.index)\n",
    "df_marketcap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIST OF COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = df_price.index\n",
    "temp_list = key_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dont include shares out because those values are repeated over many days\n",
    "## MAYBE ^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'df_price', 'df_volume', 'df_returns', 'df_shares_out', 'df_spread', 'df_market_cap'\n",
    "#### 15 pairs (6 choose 2) for each of the companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each company:\n",
    "<br>\n",
    "    compute each of the 16 pair wise correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df_price, df_volume, df_returns, df_sharesout, df_spread, df_marketcap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name_list = ['df_price', 'df_volume', 'df_returns', 'df_sharesout', 'df_spread', 'df_marketcap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df_price, df_returns, df_sharesout, df_spread, df_marketcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df  = pd.DataFrame([])\n",
    "for k in range(len(key_list)):\n",
    "    corr_dic = {}\n",
    "    ## pick the symbol name\n",
    "    symbol_name = key_list[k]\n",
    "    for i in range(len(df_name_list)):\n",
    "        for j in range(len(df_name_list)):\n",
    "                if i>=j:\n",
    "                    pass\n",
    "                else:                \n",
    "                    temp_corr = np.corrcoef(df_list[i].loc[symbol_name, :], df_list[j].loc[symbol_name, :])[0][1]\n",
    "                    temp_name = df_name_list[i].split('_')[1] + '~' + df_name_list[j].split('_')[1]\n",
    "                    corr_dic[temp_name] = temp_corr\n",
    "    temp_df = pd.DataFrame(corr_dic, index=[symbol_name])\n",
    "    corr_df = pd.concat([corr_df, temp_df])\n",
    "    \n",
    "\n",
    "print(corr_df.shape)\n",
    "corr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spread_price = df_spread/df_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "print('hi'\n",
    "(time()-t0)/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### or the average correlation accross 20 day time periods from 2007 - 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "for symbol in df_spread_price.T.columns:\n",
    "    temp_cor = np.corrcoef(df_spread_price.T[symbol], df_price.T[symbol])[0][1]\n",
    "    plt.scatter(df_spread_price.T[symbol], df_price.T[symbol])\n",
    "    plt.title('Spr/Pri ~ Price:{}\\n{}'.format(temp_cor, symbol))\n",
    "    plt.show()\n",
    "    t1 = time()\n",
    "#np.corrcoef(df_price.T['ORCL_ORACLE CORP'], df_spread_price.T['ORCL_ORACLE CORP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(3, 5, figsize=(20,10))\n",
    "# for i in range(corr_df.shape[1]):\n",
    "#     temp_series = corr_df.iloc[:, i]\n",
    "#     if i <5:\n",
    "#         for k in range(5):\n",
    "#             axs[0, k].hist(temp_series)\n",
    "#             axs[0, k].set_title('Histogram of: {}'.format(corr_df.columns[i]))\n",
    "#     elif i <10:\n",
    "#         for k in range(5):\n",
    "#             axs[1, k].hist(temp_series)\n",
    "#             axs[1, k].set_title('Histogram of: {}'.format(corr_df.columns[i]))\n",
    "#     else:\n",
    "#         for k in range(5):\n",
    "#             axs[2, k].hist(temp_series)\n",
    "#             axs[2, k].set_title('Histogram of: {}'.format(corr_df.columns[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(corr_list, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The idea is:\n",
    "## Given a company, what were its pairwise correlations between each Metric?\n",
    "## 6 metrics, 15 *distinct* pairs, 388 datapoints *per* pair of metrics, 15 histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,18))\n",
    "for i in range(corr_df.shape[1]):\n",
    "    temp_series = corr_df.iloc[:, i]\n",
    "    plt.subplot(3, 5, i+1)\n",
    "    plt.hist(temp_series, bins=20)\n",
    "    plt.xlim(-1,1)\n",
    "    plt.title(corr_df.columns[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check pairsise correlations\n",
    "## for each company check the correlation between each of the variables and plot charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot histogram of correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The follow up:\n",
    "## After we've combined all of the dataframes, what is the ONE, 1, distribution of the pairwise correlations between dates (whoa nelly)\n",
    "## 6*3000 metrics (each date's metric), 6*3000C2 pairs, 388 datapoints, 1 Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### so far it looks like marketcap could be taken out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df_price, df_volume, df_returns, df_sharesout, df_spread, df_marketcap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_price = df_list[0]\n",
    "# df_volume = df_list[1]\n",
    "# df_returns = df_list[2]\n",
    "# df_sharesout = df_list[3]\n",
    "# df_spread = df_list[4]\n",
    "# df_marketcap = df_list[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### append unique identifier to columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "for value in df_price.columns:\n",
    "    temp_value = value + '_price'\n",
    "    temp_list.append(temp_value)\n",
    "\n",
    "df_price.columns = temp_list\n",
    "df_price.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "for value in df_volume.columns:\n",
    "    temp_value = value + '_volume'\n",
    "    temp_list.append(temp_value)\n",
    "\n",
    "df_volume.columns = temp_list\n",
    "df_volume.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "for value in df_returns.columns:\n",
    "    temp_value = value + '_returns'\n",
    "    temp_list.append(temp_value)\n",
    "\n",
    "df_returns.columns = temp_list\n",
    "df_returns.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sharesout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "for value in df_sharesout.columns:\n",
    "    temp_value = value + '_sharesout'\n",
    "    temp_list.append(temp_value)\n",
    "\n",
    "df_sharesout.columns = temp_list\n",
    "df_sharesout.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "for value in df_spread.columns:\n",
    "    temp_value = value + '_spread'\n",
    "    temp_list.append(temp_value)\n",
    "\n",
    "df_spread.columns = temp_list\n",
    "df_spread.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### marketcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "for value in df_marketcap.columns:\n",
    "    temp_value = value + '_marketcap'\n",
    "    temp_list.append(temp_value)\n",
    "\n",
    "df_marketcap.columns = temp_list\n",
    "df_marketcap.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concatenate the dataframes together to form superdataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(df_price, df_volume, left_index=True, right_index=True), \n",
    "                                    df_returns, left_index=True, right_index=True),\n",
    "                                    df_sharesout, left_index=True, right_index=True), \n",
    "                                    df_spread, left_index=True, right_index=True), \n",
    "                                    df_marketcap, left_index=True, right_index=True)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(full_df.columns).to_csv('data\\\\external\\\\full_columns.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do the same for each date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each date column:\n",
    "<br>\n",
    "compute the pair-wise correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION:<br><br>which companies had the highest composite correlation? Absolute sum of correlation matrix?<br><br>what does *that* histogram look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(full_df.iloc[:, 0], full_df.iloc[:, 1])[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### this function takes way too much time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @timing\n",
    "# def corr_dates():    \n",
    "#     corr_dates = []\n",
    "#     for i in range(len(full_df.columns)):\n",
    "#         for j in range(len(full_df.columns)):\n",
    "#             if i <=j:\n",
    "#                 temp_value = np.corrcoef(full_df.iloc[:, i], full_df.iloc[:, j])[0][1]\n",
    "#                 corr_dates.append(temp_value)\n",
    "#             else:\n",
    "#                 pass\n",
    "#     return corr_dates\n",
    "\n",
    "\n",
    "# corr_dates = corr_dates()\n",
    "# print(len(corr_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#determine if any variables need to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next idea:\n",
    "## Gien a distribution of a set of correlations, which metrics should we leave out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
